{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12574313,"sourceType":"datasetVersion","datasetId":7941120}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q datasets accelerate evaluate trl accelerate bitsandbytes peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:14:26.837885Z","iopub.execute_input":"2025-07-25T11:14:26.838162Z","iopub.status.idle":"2025-07-25T11:15:48.434709Z","shell.execute_reply.started":"2025-07-25T11:14:26.838138Z","shell.execute_reply":"2025-07-25T11:15:48.433813Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install trl==0.9.6\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:16:17.034700Z","iopub.status.idle":"2025-07-25T11:16:17.034929Z","shell.execute_reply.started":"2025-07-25T11:16:17.034824Z","shell.execute_reply":"2025-07-25T11:16:17.034834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U datasets transformers pyarrow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:15:48.436876Z","iopub.execute_input":"2025-07-25T11:15:48.437123Z","iopub.status.idle":"2025-07-25T11:16:08.082329Z","shell.execute_reply.started":"2025-07-25T11:15:48.437101Z","shell.execute_reply":"2025-07-25T11:16:08.081451Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nCollecting datasets\n  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nCollecting transformers\n  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (19.0.1)\nCollecting pyarrow\n  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, transformers, datasets\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.6.0\n    Uninstalling datasets-3.6.0:\n      Successfully uninstalled datasets-3.6.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-4.0.0 pyarrow-21.0.0 transformers-4.53.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Paste your HF token when prompted (required to access Mistral 7B Instruct v2)\n\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:18:30.639419Z","iopub.execute_input":"2025-07-25T11:18:30.639660Z","iopub.status.idle":"2025-07-25T11:18:30.653551Z","shell.execute_reply.started":"2025-07-25T11:18:30.639643Z","shell.execute_reply":"2025-07-25T11:18:30.652794Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c18e94731f64a98854da87580e026b4"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:23:43.411441Z","iopub.execute_input":"2025-07-25T11:23:43.412077Z","iopub.status.idle":"2025-07-25T11:23:44.655937Z","shell.execute_reply.started":"2025-07-25T11:23:43.412047Z","shell.execute_reply":"2025-07-25T11:23:44.655099Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:18:40.403027Z","iopub.execute_input":"2025-07-25T11:18:40.403630Z","iopub.status.idle":"2025-07-25T11:21:59.444057Z","shell.execute_reply.started":"2025-07-25T11:18:40.403592Z","shell.execute_reply":"2025-07-25T11:21:59.443168Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43cf4a07c04473388285468c6bfb6f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6064c78dfd184de08d365ff6e7b12efe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0156fc8ac30a44d98732a1b5f21d1657"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf1786b69d24ffabb532e46e9f26fe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0219b71fb6de43ce92468cb2bd853ecf"}},"metadata":{}},{"name":"stderr","text":"2025-07-25 11:18:48.599016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753442328.802433      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753442328.862533      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"923f7fea88e748c296f2e9607a0bdb73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07abb128083642e9a67bd952c0203d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae6ea9e415f449fbb9e69cbc5f053d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b1587bdb821464e92df0c170bbc60c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66232af11e0749399758f71bc8305f34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016502a1355b4865ad2bc1842e44c46f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b47251b33042e0a6387e00fe679063"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1581137741.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing_enable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'prepare_model_for_kbit_training' is not defined"],"ename":"NameError","evalue":"name 'prepare_model_for_kbit_training' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:22:39.995269Z","iopub.execute_input":"2025-07-25T11:22:39.995907Z","iopub.status.idle":"2025-07-25T11:22:40.457285Z","shell.execute_reply.started":"2025-07-25T11:22:39.995880Z","shell.execute_reply":"2025-07-25T11:22:40.456750Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import json\nfrom datasets import Dataset\n\njsonl_path = \"/kaggle/input/hrsaample/100 samples.jsonl\"\n\ndata = []\nwith open(jsonl_path, 'r', encoding='utf-8') as f:\n    for line in f:\n        line = line.strip()\n        if line:  # ✅ skip empty lines\n            data.append(json.loads(line))\n\ndataset = Dataset.from_list(data)\n\n\n\n\nfrom datasets import Dataset, DatasetDict\nimport random\n\ndataset = dataset.shuffle(seed=42)\n\n\nsplit_idx = int(0.95 * len(dataset))\n\n\ntrain_dataset = dataset.select(range(split_idx))\ntest_dataset = dataset.select(range(split_idx, len(dataset)))\n\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"test\": test_dataset\n})\n\nprint(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:22:41.769933Z","iopub.execute_input":"2025-07-25T11:22:41.770209Z","iopub.status.idle":"2025-07-25T11:22:42.430653Z","shell.execute_reply.started":"2025-07-25T11:22:41.770188Z","shell.execute_reply":"2025-07-25T11:22:42.429840Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['messages'],\n        num_rows: 94\n    })\n    test: Dataset({\n        features: ['messages'],\n        num_rows: 5\n    })\n})\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def format_and_tokenize(example, tokenizer):\n    formatted_text = tokenizer.apply_chat_template(\n        example[\"messages\"],\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    return {\"text\": formatted_text}\n# Set globally ONCE\ntokenizer.pad_token = tokenizer.eos_token\n\nprocessed_train = dataset[\"train\"].map(\n    format_and_tokenize,\n    fn_kwargs={\"tokenizer\": tokenizer},\n    remove_columns=[\"messages\"],\n    num_proc=4\n)\n\nprocessed_test = dataset[\"test\"].map(\n    format_and_tokenize,\n    fn_kwargs={\"tokenizer\": tokenizer},\n    remove_columns=[\"messages\"],\n    num_proc=4\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:24:21.672049Z","iopub.execute_input":"2025-07-25T11:24:21.672561Z","iopub.status.idle":"2025-07-25T11:24:23.473285Z","shell.execute_reply.started":"2025-07-25T11:24:21.672535Z","shell.execute_reply":"2025-07-25T11:24:23.472425Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/94 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a857d18ddc42dfb7a71080e196a30a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff5e37c1c8245498e3b8700721a96c1"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"processed_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:24:25.104844Z","iopub.execute_input":"2025-07-25T11:24:25.105642Z","iopub.status.idle":"2025-07-25T11:24:25.110876Z","shell.execute_reply.started":"2025-07-25T11:24:25.105613Z","shell.execute_reply":"2025-07-25T11:24:25.110272Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 94\n})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"def tokenize_function(example):\n    tokens = tokenizer(\n        example[\"text\"],\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\",\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\ntokenized_train = processed_train.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"],\n    num_proc=4\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:25:42.230432Z","iopub.execute_input":"2025-07-25T11:25:42.230695Z","iopub.status.idle":"2025-07-25T11:25:42.837315Z","shell.execute_reply.started":"2025-07-25T11:25:42.230677Z","shell.execute_reply":"2025-07-25T11:25:42.836687Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/94 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e32c3aad4444c59a44edbcfd9d424a2"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,  # rank\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:36:26.131574Z","iopub.execute_input":"2025-07-25T11:36:26.131858Z","iopub.status.idle":"2025-07-25T11:36:26.241851Z","shell.execute_reply.started":"2025-07-25T11:36:26.131836Z","shell.execute_reply":"2025-07-25T11:36:26.241239Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom trl import SFTTrainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./mistral-7b-hr-finetuned\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    bf16=True,\n    logging_steps=10,\n    save_steps=100,\n    save_total_limit=2,\n    report_to=\"tensorboard\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_train,\n    args=training_args\n)\n\ntrainer.train()\n\ntrainer.save_model(\"./mistral-7b-hr-finetuned\")\ntokenizer.save_pretrained(\"./mistral-7b-hr-finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:25:53.106886Z","iopub.execute_input":"2025-07-25T11:25:53.107609Z","iopub.status.idle":"2025-07-25T11:26:03.313728Z","shell.execute_reply.started":"2025-07-25T11:25:53.107576Z","shell.execute_reply":"2025-07-25T11:26:03.312755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/94 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d07a23ab584c75a923585db818293d"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3967603170.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./mistral-7b-hr-finetuned\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3749\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;31m# Calculate accuracy only on non-padding tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m             \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mcorrect_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0"],"ename":"RuntimeError","evalue":"The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"for name,module in model.named_modules():\n    if isinstance(module,torch.nn.Linear):\n        print(name)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:39:38.601148Z","iopub.execute_input":"2025-07-25T11:39:38.601824Z","iopub.status.idle":"2025-07-25T11:39:38.610340Z","shell.execute_reply.started":"2025-07-25T11:39:38.601795Z","shell.execute_reply":"2025-07-25T11:39:38.609553Z"}},"outputs":[{"name":"stdout","text":"base_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.0.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.1.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.2.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.3.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.4.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.5.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.6.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.7.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.8.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.9.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.10.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.11.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.12.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.13.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.14.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.15.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.16.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.17.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.18.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.19.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.20.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.21.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.22.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.23.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.24.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.25.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.26.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.27.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.28.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.29.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.30.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.q_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.k_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.v_proj.base_layer\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.self_attn.o_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.mlp.gate_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.mlp.up_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.model.layers.31.mlp.down_proj\nbase_model.model.base_model.model.base_model.model.base_model.model.base_model.model.lm_head\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\n\n# 1️⃣ Load model & tokenizer\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# 2️⃣ Apply PEFT (LoRA) with BEST LAYERS\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\n        f\"model.layers.{i}.self_attn.q_proj\" for i in [0, 4, 8, 12, 16, 20, 23]\n    ] + [\n        f\"model.layers.{i}.self_attn.k_proj\" for i in [0, 4, 8, 12, 16, 20, 23]\n    ] + [\n        f\"model.layers.{i}.self_attn.v_proj\" for i in [0, 4, 8, 12, 16, 20, 23]\n    ] + [\n        f\"model.layers.{i}.self_attn.o_proj\" for i in [0, 4, 8, 12, 16, 20, 23]\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # ✅ Check trainable parameter count\n\n# 3️⃣ Tokenization\ndef tokenize_function(example):\n    tokens = tokenizer(\n        example[\"text\"],\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\",\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\ntokenized_train = processed_train.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"],\n    num_proc=4\n)\n\n# 4️⃣ TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./mistral-peft-hr-finetuned\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=3,\n    bf16=True,\n    logging_steps=10,\n    save_steps=100,\n    save_total_limit=2,\n    report_to=\"tensorboard\"\n)\n\n# 5️⃣ Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_train,\n    args=training_args\n)\n\n# 6️⃣ Train\ntrainer.train()\n\n# 7️⃣ Save PEFT-adapted model\ntrainer.model.save_pretrained(\"./mistral-peft-hr-finetuned\")\ntokenizer.save_pretrained(\"./mistral-peft-hr-finetuned\")\n\nprint(\"✅ PEFT (LoRA) fine-tuning complete and saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T11:44:43.707486Z","iopub.execute_input":"2025-07-25T11:44:43.708240Z","iopub.status.idle":"2025-07-25T11:44:57.929073Z","shell.execute_reply.started":"2025-07-25T11:44:43.708213Z","shell.execute_reply":"2025-07-25T11:44:57.927692Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e300cfb4ff3c478081888902c4319b0c"}},"metadata":{}},{"name":"stdout","text":"trainable params: 2,981,888 || all params: 7,244,713,984 || trainable%: 0.0412\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/94 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0dde35f58cf4a1fb56e5bada2c0b792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/94 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80832e147a86485c8e3bc2b2edb0abb6"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2929401927.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# 6️⃣ Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# 7️⃣ Save PEFT-adapted model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3749\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    824\u001b[0m         \"\"\"\n\u001b[1;32m    825\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         (loss, outputs) = super().compute_loss(\n\u001b[0m\u001b[1;32m    827\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3834\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3835\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3836\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3837\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3838\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1758\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    419\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 1 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 4226 has 14.71 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 26.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 32.00 MiB. GPU 1 has a total capacity of 14.74 GiB of which 26.12 MiB is free. Process 4226 has 14.71 GiB memory in use. Of the allocated memory 14.56 GiB is allocated by PyTorch, and 26.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}